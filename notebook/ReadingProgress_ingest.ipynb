{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Progress Module - Ingest\r\n",
        "\r\n",
        "This notebook demonstrates the utility of the OEA_py class notebook, and speeding up the process of ingesting the Insights/Reading Progress data.\r\n",
        "\r\n",
        "The steps outlined below describe how this notebook is used to ingest the Microsoft Education Insights module tables:\r\n",
        "\r\n",
        "- Set the workspace for where the tables are located. \r\n",
        "- 1 function is defined and used:\r\n",
        "   1. **ingest_reading_prog**: identifies primary keys per table and ingests each table from Insights (except AadGroupMembership, PersonRelationship, and RefTranslation)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace = 'dev'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "75",
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-13T17:25:01.3723354Z",
              "session_start_time": "2023-01-13T17:25:01.4146117Z",
              "execution_start_time": "2023-01-13T17:28:41.1660616Z",
              "execution_finish_time": "2023-01-13T17:28:41.3313447Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 75, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "75",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-13T17:25:03.0585236Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-13T17:28:47.3418784Z",
              "execution_finish_time": "2023-01-13T17:28:47.3422331Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(, 75, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-13 17:28:45,990 - OEA - INFO - Now using workspace: dev\n2023-01-13 17:28:45,991 - OEA - INFO - OEA initialized.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace(workspace)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "75",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-13T17:25:05.3624402Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-13T17:28:47.5848012Z",
              "execution_finish_time": "2023-01-13T17:28:47.7739201Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 75, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-13 17:28:47,566 - OEA - INFO - Now using workspace: dev\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) this method is almost identical to the ingest function in the OEA framework, except with the additional function to change the ingested directory \r\n",
        "def ingest_reading_prog(entity_path, write_entity_path, primary_key='id', options={}):\r\n",
        "    \"\"\" Ingests the data for the entity in the given path.\r\n",
        "        CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\r\n",
        "        To specify options that are different from these defaults, use the options param.\r\n",
        "        eg, ingest('contoso_sis/v0.1/students') # ingests all entities found in that path\r\n",
        "        eg, ingest('contoso_sis/v0.1/students', options={'header':False}) # for CSV files that don't have a header\r\n",
        "    \"\"\"\r\n",
        "    primary_key = oea.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
        "    ingested_path = f'stage2/Ingested/{write_entity_path}'\r\n",
        "    raw_path = f'stage1/Transactional/{entity_path}'\r\n",
        "    batch_type, source_data_format = oea.get_batch_info(raw_path)\r\n",
        "    logger.info(f'Ingesting from: {raw_path}, batch type of: {batch_type}, source data format of: {source_data_format}')\r\n",
        "    source_url = oea.to_url(f'{raw_path}/{batch_type}_batch_data')\r\n",
        "\r\n",
        "    if batch_type == 'snapshot': source_url = f'{source_url}/{oea.get_latest_folder(source_url)}' \r\n",
        "            \r\n",
        "    logger.debug(f'Processing {batch_type} data from: {source_url} and writing out to: {ingested_path}')\r\n",
        "    if batch_type == 'snapshot':\r\n",
        "        def batch_func(df): oea.overwrite(df, ingested_path, primary_key)\r\n",
        "    elif batch_type == 'additive':\r\n",
        "        def batch_func(df): oea.append(df, ingested_path, primary_key)\r\n",
        "    elif batch_type == 'delta':\r\n",
        "        def batch_func(df): oea.upsert(df, ingested_path, primary_key)\r\n",
        "    else:\r\n",
        "        raise ValueError(\"No valid batch folder was found at that path (expected to find a single folder with one of the following names: snapshot_batch_data, additive_batch_data, or delta_batch_data). Are you sure you have the right path?\")                      \r\n",
        "\r\n",
        "    if options == None: options = {}\r\n",
        "    options['format'] = source_data_format # eg, 'csv', 'json'\r\n",
        "    if source_data_format == 'csv' and (not 'header' in options or options['header'] == None): options['header'] = True  # default to expecting a header in csv files\r\n",
        "\r\n",
        "    number_of_new_inbound_rows = oea.process(source_url, batch_func, options)\r\n",
        "    if number_of_new_inbound_rows > 0:    \r\n",
        "        oea.add_to_lake_db(ingested_path)\r\n",
        "    return number_of_new_inbound_rows"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "75",
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-13T18:20:19.2820679Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-13T18:20:19.4045271Z",
              "execution_finish_time": "2023-01-13T18:20:19.5655732Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 75, 17, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) The next step is to ingest the batch data into stage2\r\n",
        "# Note that when you run this the first time, you'll see an info message like \"Number of new inbound rows processed: 2\".\r\n",
        "# If you run this a second time, the number of inbound rows processed will be 0 because the ingestion uses spark structured streaming to keep track of what data has already been processed.\r\n",
        "options = {'header':False}\r\n",
        "ingest_reading_prog(f'M365/v1.14/activity', f'reading_progress/v0.1/activity', '_c3', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/AadGroup', f'reading_progress/v0.1/AadGroup', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/AadUser', f'reading_progress/v0.1/AadUser', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/AadUserPersonMapping', f'reading_progress/v0.1/AadUserPersonMapping', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Course', f'reading_progress/v0.1/Course', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/CourseGradeLevel', f'reading_progress/v0.1/CourseGradeLevel', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/CourseSubject', f'reading_progress/v0.1/CourseSubject', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Enrollment', f'reading_progress/v0.1/Enrollment', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Organization', f'reading_progress/v0.1/Organization', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Person', f'reading_progress/v0.1/Person', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonDemographic', f'reading_progress/v0.1/PersonDemographic', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonDemographicEthnicity', f'reading_progress/v0.1/PersonDemographicEthnicity', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonDemographicPersonFlag', f'reading_progress/v0.1/PersonDemographicPersonFlag', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonDemographicRace', f'reading_progress/v0.1/PersonDemographicRace', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonEmailAddress', f'reading_progress/v0.1/PersonEmailAddress', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonIdentifier', f'reading_progress/v0.1/PersonIdentifier', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonOrganizationRole', f'reading_progress/v0.1/PersonOrganizationRole', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonPhoneNumber', f'reading_progress/v0.1/PersonPhoneNumber', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/RefDefinition', f'reading_progress/v0.1/RefDefinition', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Section', f'reading_progress/v0.1/Section', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/SectionGradeLevel', f'reading_progress/v0.1/SectionGradeLevel', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/SectionSession', f'reading_progress/v0.1/SectionSession', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/SectionSubject', f'reading_progress/v0.1/SectionSubject', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Session', f'reading_progress/v0.1/Session', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/SourceSystem', f'reading_progress/v0.1/SourceSystem', '_c0', options)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": "Used for \"2_ingest_reading_progress\" pipeline.",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}