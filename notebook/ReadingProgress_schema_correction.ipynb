{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Progress Module Ingestion - Schema Correction\r\n",
        "\r\n",
        "This notebook demonstrates the utility of the OEA_py class notebook, while correcting module tables initially ingested without headers and incorrect data types.\r\n",
        "\r\n",
        "Tables are read from ```stage2/Ingested/reading_progress/v0.1``` and written out, with the corrected schema, to ```stage2/Ingested_Corrected/reading_progress/v0.1```\r\n",
        "\r\n",
        "The steps outlined below describe how this notebook is used to correct the Microsoft Education Insights module tables:\r\n",
        "- Set the workspace for where the table schemas are to be corrected. \r\n",
        "- 4 functions are defined and used:\r\n",
        "   1. **_extract_element**: uses the Insights metadata to extract the correct column names.\r\n",
        "   2. **_dtype_config**: uses the Insights metadata to extract the correct column dtypes.\r\n",
        "   3. **correct_insights_table_schema**: uses the corrected column names and dtypes to correct the schema per table given to the function.\r\n",
        "   4. **correct_reading_progress_dataset**: extracts the names of all the folders currently stored in stage2/Ingested/reading_progress, corrects the schema per table using the function above, and overwrites the tables with the updated schemas.\r\n",
        "   "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace = 'dev'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "90",
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-13T20:09:41.4611724Z",
              "session_start_time": "2023-01-13T20:09:41.5648499Z",
              "execution_start_time": "2023-01-13T20:13:06.4177579Z",
              "execution_finish_time": "2023-01-13T20:13:06.6344269Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 90, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "90",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-13T20:09:41.4628401Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-13T20:13:11.4120393Z",
              "execution_finish_time": "2023-01-13T20:13:11.4122508Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(, 90, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-13 20:13:11,118 - OEA - INFO - Now using workspace: dev\n2023-01-13 20:13:11,119 - OEA - INFO - OEA initialized.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace(workspace)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "90",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-13T20:09:42.839638Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-13T20:13:11.5181937Z",
              "execution_finish_time": "2023-01-13T20:13:11.7350179Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 90, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-13 20:13:11,492 - OEA - INFO - Now using workspace: dev\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) schema correction, since Insights data initially landed doesn't have column headers\r\n",
        "\r\n",
        "def _extract_element(lst, element_num=0):\r\n",
        "    return [item[element_num] for item in lst]\r\n",
        "\r\n",
        "def _dtype_config(dtype_lst):\r\n",
        "    return [item.capitalize() + 'Type()' for item in dtype_lst]\r\n",
        "\r\n",
        "def correct_insights_table_schema(df, table_name):\r\n",
        "    list_of_column_names = _extract_element(metadata[table_name])\r\n",
        "    list_of_column_dtypes = _extract_element(metadata[table_name], 1)\r\n",
        "    list_of_column_dtypes = _dtype_config(list_of_column_dtypes)\r\n",
        "\r\n",
        "    n = 0\r\n",
        "    df_updatedColumns = df\r\n",
        "    for c in df.columns:\r\n",
        "        if c != 'rundate':\r\n",
        "            new_col_name = list_of_column_names[n]\r\n",
        "            df_updatedColumns = df_updatedColumns.withColumnRenamed(c, new_col_name)\r\n",
        "            if list_of_column_dtypes[n] != 'StringType()':\r\n",
        "                if list_of_column_dtypes[n] == 'IntegerType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(IntegerType()))\r\n",
        "                elif list_of_column_dtypes[n] == 'TimestampType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(TimestampType()))\r\n",
        "                elif list_of_column_dtypes == 'ShortType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(ShortType()))\r\n",
        "                elif list_of_column_dtypes[n] == 'LongType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(LongType()))\r\n",
        "                elif list_of_column_dtypes[n] == 'DoubleType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(DoubleType()))\r\n",
        "                elif list_of_column_dtypes[n] == 'DateType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(DateType()))\r\n",
        "                elif list_of_column_dtypes[n] == 'BooleanType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(BooleanType()))\r\n",
        "        else:\r\n",
        "            df_updatedColumns = df_updatedColumns\r\n",
        "        n = n + 1\r\n",
        "    return df_updatedColumns"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "90",
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-13T20:09:47.2400931Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-13T20:13:11.8601106Z",
              "execution_finish_time": "2023-01-13T20:13:12.0284732Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 90, 4, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_reading_progress_dataset(tables_source, write_destination):\r\n",
        "    items = oea.get_folders(tables_source)\r\n",
        "    for item in items: \r\n",
        "        if item == 'metadata.csv':\r\n",
        "            logger.info('ignore metadata processing, since this is not a table to be ingested')\r\n",
        "        else:\r\n",
        "            table_path = tables_source +'/'+ item\r\n",
        "            spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "            streaming_df = spark.readStream.format('delta').load(oea.to_url(table_path))\r\n",
        "            df_corrected = correct_insights_table_schema(streaming_df, table_name=item)\r\n",
        "            query = df_corrected.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(table_path) + '/_checkpoints')\r\n",
        "            query = query.start(oea.to_url(write_destination + '/' +item))\r\n",
        "            query.awaitTermination() \r\n",
        "            logger.info('Successfully corrected the schema for table: ' + item + ' from: ' + table_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "90",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-13T20:13:16.7318995Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-13T20:13:16.8988975Z",
              "execution_finish_time": "2023-01-13T20:13:17.0596518Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 90, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/metadata.csv')\r\n",
        "correct_reading_progress_dataset('stage2/Ingested/reading_progress/v0.1', 'stage2/Ingested_Corrected/reading_progress/v0.1')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2med",
              "session_id": "90",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-01-13T20:13:24.0782346Z",
              "session_start_time": null,
              "execution_start_time": "2023-01-13T20:13:24.188172Z",
              "execution_finish_time": "2023-01-13T20:14:24.4695926Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2med, 90, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-13 20:14:20,607 - OEA - INFO - Successfully corrected the schema for table: AadGroup from: stage2/Ingested/M365/v1.14/AadGroup\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-411ef57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/metadata.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/cstohlmann/oea-insights-module/main/test_data/metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcorrect_insights_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stage2/Ingested/M365/v1.14'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stage2/Ingested_Corrected/M365/v1.14'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-411ef57a>\u001b[0m in \u001b[0;36mcorrect_insights_dataset\u001b[0;34m(tables_source, write_destination)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"set spark.sql.streaming.schemaInference=true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mstreaming_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mdf_corrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_insights_table_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreaming_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_corrected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpointLocation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/_checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_destination\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-411ef57a>\u001b[0m in \u001b[0;36mcorrect_insights_table_schema\u001b[0;34m(df, table_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'rundate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mnew_col_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_of_column_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mdf_updatedColumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_updatedColumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_col_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlist_of_column_dtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'StringType()'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('delta').load(oea.to_url('stage2/Ingested_Corrected/reading_progress/v0.1/activity'), header='true')\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": "Used for \"2_ingest_reading_progress\" pipeline. Corrects the schema from Insights data originally landed - updating column names and data types from the metadata.",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}