{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Progress Module - Refine\r\n",
        "\r\n",
        "This notebook demonstrates the utility of the OEA_py class notebook, and speeding up the process of refining/pseudonymizing the Reading Progress data. \r\n",
        "\r\n",
        "The steps outlined below describe how this notebook is used to refine tables originally from the Microsoft Education Insights data source:\r\n",
        "\r\n",
        "- Set the workspace for where the tables are located. \r\n",
        "- 1 function is defined and used:\r\n",
        "   1. **refine_reading_prog_corrected**: similar to the ```oea.refine()``` function, except reads from ```stage2/Ingested_Corrected``` rather than ```stage2/Ingested```.\r\n",
        "      * *Note*: This function currently does not accomodate processing for change data over time; it is expected to be updated for production purposes.\r\n",
        "   2. **refine_reading_progress_dataset**: \r\n",
        "      * transforms/aggregates data from Insights tables into two tables relevant for the Reading_Progress.\r\n",
        "      * uses the Reading_Progress module metadata.csv to pseudonymize each table according to whether it is to be hashed, masked, or has no-operation."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace = 'dev'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace(workspace)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) this step refines the data through the use of metadata (this is where the pseudonymization of the data occurs).\r\n",
        "def refine_reading_prog_corrected(df, table_name, metadata=None, primary_key='id'):\r\n",
        "    source_path = f'stage2/Ingested_Corrected/reading_progress/v0.1/activity'\r\n",
        "    sink_general_path = f'stage2/Refined/reading_progress/v0.1/general/{table_name}'\r\n",
        "    sink_sensitive_path = f'stage2/Refined/reading_progress/v0.1/sensitive/{table_name}_lookup'\r\n",
        "\r\n",
        "    # NOTE: Currently does not accomodate change data; this is expected to be updated for production purposes\r\n",
        "    #df_changes = oea.get_latest_changes(source_path, sink_general_path)\r\n",
        "    spark_schema = oea.to_spark_schema(metadata)\r\n",
        "    df = oea.modify_schema(df, spark_schema)        \r\n",
        "\r\n",
        "    if df.count() > 0:\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\r\n",
        "        oea.upsert(df_pseudo, sink_general_path, primary_key)\r\n",
        "        oea.upsert(df_lookup, sink_sensitive_path, primary_key)\r\n",
        "        oea.add_to_lake_db(sink_general_path)\r\n",
        "        oea.add_to_lake_db(sink_sensitive_path)\r\n",
        "        logger.info(f'Processed {df.count()} rows from {source_path} into stage2/Refined')\r\n",
        "    else:\r\n",
        "        logger.info(f'No updated rows in {source_path} to process.')\r\n",
        "    return df.count()\r\n",
        "\r\n",
        "def refine_reading_progress_dataset(tables_source):\r\n",
        "    # read in relevant tables for data transformation\r\n",
        "    base_path = tables_source\r\n",
        "    df_activity = oea.load(base_path + '/activity')\r\n",
        "    df_aaduserpersonmapping = oea.load(base_path + '/AadUserPersonMapping')\r\n",
        "    df_person = oea.load(base_path + '/Person/')\r\n",
        "    df_personOrgRole = oea.load(base_path + '/PersonOrganizationRole')\r\n",
        "    df_organization = oea.load(base_path + '/Organization')\r\n",
        "    df_refDefinition = oea.load(base_path + '/RefDefinition')\r\n",
        "    # separate student frame, subset and refine data\r\n",
        "    dfStudent = df_personOrgRole.join(df_person, df_personOrgRole.PersonId == df_person.Id, how='inner')\r\n",
        "    dfStudent = dfStudent.select('PersonId', 'Surname', 'GivenName', 'MiddleName', 'RefRoleId', 'RefGradeLevelId', 'OrganizationId')\r\n",
        "    dfStudent = dfStudent.join(df_organization, dfStudent.OrganizationId == df_organization.Id, how='inner').withColumnRenamed('Name', 'OrganizationName')\r\n",
        "    dfStudent = dfStudent.select('PersonId', 'Surname', 'GivenName', 'MiddleName', 'RefRoleId', 'RefGradeLevelId', 'OrganizationId', 'OrganizationName')\r\n",
        "    dfStudent = dfStudent.join(df_refDefinition, dfStudent.RefRoleId == df_refDefinition.Id, how='inner').withColumnRenamed('Code', 'PersonRole')\r\n",
        "    dfStudent = dfStudent.select('PersonId', 'Surname', 'GivenName', 'MiddleName', 'PersonRole', 'RefGradeLevelId', 'OrganizationId', 'OrganizationName')\r\n",
        "    dfStudent = dfStudent.filter(dfStudent['PersonRole'] == 'Student')\r\n",
        "    dfStudent = dfStudent.join(df_refDefinition, dfStudent.RefGradeLevelId == df_refDefinition.Id, how='left').withColumnRenamed('Code', 'StudentGrade')\r\n",
        "    dfStudent = dfStudent.select('PersonId', 'Surname', 'GivenName', 'MiddleName', 'PersonRole', 'StudentGrade', 'OrganizationId', 'OrganizationName')\r\n",
        "    df_aaduserpersonmapping = df_aaduserpersonmapping.withColumnRenamed('PersonId', 'id')\r\n",
        "    dfStudent = dfStudent.join(df_aaduserpersonmapping, dfStudent.PersonId == df_aaduserpersonmapping.id, how='inner').withColumnRenamed('ObjectId', 'AadUserId')\r\n",
        "    dfStudent = dfStudent.select('PersonId', 'AadUserId', 'Surname', 'GivenName', 'MiddleName', 'PersonRole', 'StudentGrade', 'OrganizationId', 'OrganizationName')\r\n",
        "\r\n",
        "    refine_reading_prog_corrected(dfStudent, 'Student', metadata['Student'], 'PersonId_pseudonym')\r\n",
        "    # refine reading progress data from Insights activity table\r\n",
        "    dfReadingProgress = df_activity.where(\"AppName == 'ReadingProgress'\")\r\n",
        "    dfReadingProgress = dfReadingProgress.select('ActorId', 'SignalId', 'SignalType', 'StartTime', 'AppName', 'Action', 'ClassId', 'ReadingSubmissionWordsPerMinute', 'ReadingSubmissionAccuracyScore', \\\r\n",
        "                                        'ReadingSubmissionRepetitionsCount', 'ReadingSubmissionInsertionsCount', 'ReadingSubmissionMispronunciationCount', 'ReadingSubmissionObmissionCount', 'ReadingSubmissionAttemptNumber', \\\r\n",
        "                                        'ReadingAssignmentWordCount', 'ReadingAssignmentFleschKincaidGradeLevel', 'ReadingAssignmentLanguage')\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumnRenamed('ActorId', 'AadUserId').withColumnRenamed('ClassId', 'AadGroupId')\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionAccuracyScore', dfReadingProgress['ReadingSubmissionAccuracyScore'].cast(DoubleType()))\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionRepetitionsRate', F.col('ReadingSubmissionRepetitionsCount')/F.col('ReadingAssignmentWordCount') * 100) \r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionRepetitionsRate', F.round(F.col('ReadingSubmissionRepetitionsRate'), 3))\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionMispronunciationRate', F.col('ReadingSubmissionMispronunciationCount')/F.col('ReadingAssignmentWordCount') * 100) \r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionMispronunciationRate', F.round(F.col('ReadingSubmissionMispronunciationRate'), 3))\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionInsertionsRate', F.col('ReadingSubmissionInsertionsCount')/F.col('ReadingAssignmentWordCount') * 100) \r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionInsertionsRate', F.round(F.col('ReadingSubmissionInsertionsRate'), 3))\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionObmissionRate', F.col('ReadingSubmissionObmissionCount')/F.col('ReadingAssignmentWordCount') * 100) \r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionObmissionRate', F.round(F.col('ReadingSubmissionObmissionRate'), 3))\r\n",
        "\r\n",
        "    try:\r\n",
        "        refine_reading_prog_corrected(dfReadingProgress, 'ReadingProgress_activity', metadata['ReadingProgress_activity'], 'SignalId')\r\n",
        "    except AnalysisException as e:\r\n",
        "        # This means the table may have not been properly refined due to errors with the primary key not aligning with columns expected in the lookup table.\r\n",
        "        pass\r\n",
        "    \r\n",
        "    logger.info('Finished refining Reading Progress tables.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Reading_Progress/data/metadata.csv')\r\n",
        "refine_reading_progress_dataset('stage2/Ingested_Corrected/reading_progress/v0.1')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": "Used for \"3_refine_reading_progress\" pipeline.",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}